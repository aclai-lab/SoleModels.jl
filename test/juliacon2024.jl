# JuliaCon2024 demo

# Load packages
# begin
#     using MLJ
#     using MLJDecisionTreeInterface
#     using DataFrames
#     using Random
# end

# Load dataset
X, y = begin
    X, y = @load_iris;
    X = DataFrame(X)
    X, y
end

# Split dataset
X_train, y_train, X_test, y_test = begin
    train, test = partition(eachindex(y), 0.8, shuffle=true, rng = Random.MersenneTwister(42));
    X_train, y_train = X[train, :], y[train];
    X_test, y_test = X[test, :], y[test];
    X_train, y_train, X_test, y_test
end;

# Train tree
mach = begin
    Tree = MLJ.@load DecisionTreeClassifier pkg=DecisionTree
    model = Tree(max_depth=-1, rng = Random.MersenneTwister(42))
    machine(model, X_train, y_train) |> fit!
end

# Inspect the tree
ðŸŒ± = fitted_params(mach).tree

# Convert to ðŸŒž-compliant model
import DecisionTree as DT
ðŸŒ² = solemodel(ðŸŒ±);

# Print model
printmodel(ðŸŒ²);

# Inspect the rules
listrules(ðŸŒ²)

# Inspect rule metrics
metricstable(ðŸŒ²)

# Inspect normalized rule metrics
metricstable(ðŸŒ², normalize = true)

# Make test instances flow into the model, so that test metrics can, then, be computed.
apply!(ðŸŒ², X_test, y_test)

# Pretty table of rules and their metrics
metricstable(ðŸŒ²; normalize = true, metrics_kwargs = (; additional_metrics = (; height = r->SoleLogics.height(antecedent(r)))))

# Join some rules for the same class into a single, sufficient and necessary condition for that class
metricstable(joinrules(ðŸŒ²; min_ncovered = 1, normalize = true))
